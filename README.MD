# Vitamin-CUDA

> **One Kernel a Day, Keeps High Latency Away.** ğŸš€

Welcome to your daily dose of CUDA programming! **Vitamin-CUDA** is a curated collection of hands-on CUDA practices, designed to take you from **Hello World** to **High Performance**. Whether you are a beginner looking to understand the grid-stride loop or an enthusiast diving into warp-level primitives, there's a kernel here for you.

ğŸ’ŠLet's get started and happy coding! ğŸ§ 

## Prerequisites ğŸ› ï¸

- NVIDIA GPU (Compute Capability 6.0+)
- CUDA Toolkit 11.0+
- C++ Compiler (GCC/Clang/MSVC)
- CMake 3.18+ (Optional, but recommended)
- PyTorch (For extension examples/python bingding and performence comparation)

I recommend using nvidia pytorch ngc docker images for a quick start! Refer to <https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch>

## Kernels (80+ kernels)

All kernels were tested on an RTX 5060 GPU (unless otherwise specified) and benchmarked against PyTorch 2.9

### Easy (ğŸŒŸ~ğŸŒŸğŸŒŸ)

- [elementwise](./kernels/elementwise): elementwise add
  - [x] elementwise_add fp32/fp16 ç‰ˆ
  - [x] elementwise_add_fp16x2(fp16å‘é‡åŒ–)
  - [x] elementwise_add_fp16x8(fp16å‘é‡åŒ–)
  - [x] elementwise_add_fp16x8(fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [sigmoid](./kernels/sigmoid)
  - [x] sigmoid fp32/fp16 ç‰ˆ
  - [x] sigmoid_fp16x2(fp16å‘é‡åŒ–)
  - [x] sigmoid_fp16x8(fp16å‘é‡åŒ–)
  - [x] sigmoid_fp16x8(fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [swish](./kernels/swish)
  - [x] swish fp32/fp16 ç‰ˆ
  - [x] swish_fp16x2(fp16å‘é‡åŒ–)
  - [x] swish_fp16x8(fp16å‘é‡åŒ–)
  - [x] swish_fp16x8(fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [relu](./kernels/relu)
  - [x] relu fp32/fp16 ç‰ˆ
  - [x] relu_fp16x2(fp16å‘é‡åŒ–)
  - [x] relu_fp16x8(fp16å‘é‡åŒ–)
  - [x] relu_fp16x8(fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [relu6](./kernels/relu6)
  - [x] relu6 fp32/fp16 ç‰ˆ
  - [x] relu6_fp16x2(fp16å‘é‡åŒ–)
  - [x] relu6_fp16x8(fp16å‘é‡åŒ–)
  - [x] relu6_fp16x8(fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [elu](./kernels/elu)
  - [x] elu fp32/fp16 ç‰ˆ
  - [x] elu_fp16x2(fp16å‘é‡åŒ–)
  - [x] elu_fp16x8(fp16å‘é‡åŒ–)
  - [x] elu_fp16x8(fp16å‘é‡åŒ–, packed r/w, half2 è¿‘ä¸¤å€æå‡)
  - [x] pytorch op bindings && diff check
- [gelu](./kernels/gelu)
  - [x] gelu fp32/fp16 ç‰ˆ
  - [x] gelu_fp16x2(fp16å‘é‡åŒ–)
  - [x] gelu_fp16x8(fp16å‘é‡åŒ–)
  - [x] gelu_fp16x8(fp16å‘é‡åŒ–ï¼Œpacked r/w)
  - [x] pytorch op bindings && diff check
- [hardswish](./kernels/hardswish)
  - [x] hardswish fp32/fp16 ç‰ˆ
  - [x] hardswish_fp16x2(fp16å‘é‡åŒ–)
  - [x] hardswish_fp16x8(fp16å‘é‡åŒ–)
  - [x] hardswish_fp16x8(fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [embedding](./kernels/embedding/)
  - [x] embedding fp32/fp16 ç‰ˆ
  - [x] embedding_fp32x4(fp32å‘é‡åŒ–)
  - [x] embedding_fp32x4(fp32å‘é‡åŒ–, packed r/w)
  - [x] embedding_fp16x2(fp16å‘é‡åŒ–)
  - [x] embedding_fp16x8(fp16å‘é‡åŒ–)
  - [x] embedding_fp16x8(fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [rope](./kernels/rope)
  - [x] pytorch naive rope
  - [x] pytorch rope with cos/sin table
  - [x] rope fp32 ç‰ˆ (æ¯”pytorch naive å®ç°å¿«ä¸€ä¸ªæ•°é‡çº§)
  - [x] rope fp32x4 ç‰ˆ (fp32å‘é‡åŒ–ï¼Œç¨å¤§è§„æ¨¡åå¿«å‡ åå€)
  - [x] pytorch op bindings && diff check

### Medium (ğŸŒŸğŸŒŸ~ğŸŒŸğŸŒŸğŸŒŸ)

- [reduce](./kernels/reduce/) : åŸºäº warp shuffle add
  - [x] reduce_sum fp32/fp16 ç‰ˆ
  - [x] reduce_sum_fp16x2(fp16å‘é‡åŒ–)
  - [x] reduce_sum_fp16x8_packed(fp16å‘é‡åŒ–, packed r/w)
  - [x] reduce_sum int8 ç‰ˆ
  - [x] reduce_sum_i8x16_packed (int8å‘é‡åŒ–ï¼Œpacked r/w)
  - [x] reduce_sum_i8x16_packed (int8å‘é‡åŒ–ï¼Œpacked r/w, dp4a, ç›¸æ¯”torchæœ´ç´ å®ç°å¿«å‡ åå€)
  - [x] reduce_sum_i8x64_packed (int8å‘é‡åŒ–ï¼Œpacked r/w, dp4a)
  - [x] pytorch op bindings && diff check
- [dot_product](./kernels/dot_product)
  - [x] dot_product fp32/fp16 ç‰ˆ
  - [x] dot_product_fp32x4(fp32å‘é‡åŒ–)
  - [x] dot_product_fp16x2(fp16å‘é‡åŒ–)
  - [x] dot_product_fp16x8(fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [softmax](./kernels/softmax/)
  - [x] safe online softmax fp32/fp16 ç‰ˆ
  - [x] safe online softmax fp32x4 ç‰ˆ (fp32å‘é‡åŒ–)
  - [x] safe online softmax fp16x8 ç‰ˆ (fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [rmsnorm](./kernels/rmsnorm/)
  - [x] naive torch rmsnorm
  - [x] rmsnorm fp32/fp16 ç‰ˆ
  - [x] rmsnorm fp32x4 ç‰ˆ (fp32å‘é‡åŒ–)
  - [x] rmsnorm_fp32x4_smem
  - [x] rmsnorm fp16x8 ç‰ˆ (fp16å‘é‡åŒ–, packed r/w)
  - [x] rmsnorm_fp16x8_smem ç‰ˆ (fp16å‘é‡åŒ–, packed r/w)
  - [x] pytorch op bindings && diff check
- [transpose](./kernels/mat_transpose/)
  - [x] transpose_coalesced_read (inputè§†è§’ï¼Œåˆå¹¶è¯»)
  - [x] transpose_coalesced_write (outputè§†è§’ï¼Œåˆå¹¶å†™)
  - [x] transpose_smem (å…±äº«å†…å­˜ç¼“å­˜ï¼Œå—çŠ¶è¯»å†™)
  - [x] transpose_smem_bcf (å…±äº«å†…å­˜æ— å†²çªç‰ˆ)
  - [x] transpose_smem_packed_bcf (å…±äº«å†…å­˜æ— å†²çªç‰ˆï¼Œfloat4å‘é‡åŒ–è¯»å†™)
  - [x] transpose_smem_swizzled_packed (å…±äº«å†…å­˜æ— å†²çªç‰ˆï¼Œfloat4å‘é‡åŒ–è¯»å†™)
  - [x] pytorch op bindings && diff check

### Hard (ğŸŒŸğŸŒŸğŸŒŸ~ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ)

- [sgemv](./kernels/sgemv/)
  - [x] gemv fp32ç‰ˆ
  - [x] gemv fp32x4ï¼ˆå‘é‡åŒ–è¯»å–ï¼‰
  - [x] pytorch op bindings && diff check
- [sgemm](./kernels/sgemm/)
  - [] sgemm fp32ç‰ˆ

## Samples

- [deviceQuery](./samples/deviceQuery)

## Reference

- [LeetCUDA](https://github.com/xlite-dev/LeetCUDA)
- [NVIDIA CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)
